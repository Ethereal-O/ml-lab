# 机器学习--Lab1

## 1. 基本介绍

### 1.1 目的
本次Lab主要是实现两种方法的SVM模型并对比其准确度和时间。其中，两种方法分别是：使用sklearn包中的线性SVM模型和使用梯度下降法的手动实现的SVM模型。

### 1.2 环境
#### 1.2.1 python环境
本次使用的python版本为3.7.9 64bit。
#### 1.2.2 包环境
通过pip freeze将环境生成requirements.txt文件，如下所示：
- numpy\==1.21.4
- scikit_learn\==1.1.3
#### 1.2.3 机器环境
理论上，本代码在所有64bit的机器上均能正常运行。本次实验所用的环境为：
- CPU: Intel(R) Core(TM) i5-6300U CPU @ 2.40GHz   2.50 GHz
- RAM: 4GB 1867MHZ

## 2. SVM-sklearn
### 2.1 实现方法
通过初始化SVM线性模型，并输入训练数据，其能自动训练。将测试数据输入模型，得到准确度。准确度的计算方式是，模型计算出的预测值与实际值差的绝对值的平均。
### 2.2 参数
经过测试与选取，我选择了如下参数：
THRESHOLD = 1e-3
MAX_ITERATION = -1
其中，选择THRESHOLD = 1e-3是为了在保证时间较短情况下增加准确度，选择MAX_ITERATION = -1是为了避免其提早结束导致准确度骤降（若选择和梯度下降相同的最大迭代次数，即8000次，则其准确度只有0.71，并抛出警告其提前结束）
### 2.3 结果
根据上述参数配置，得到结果如下：
train_acc: 0.975
test_acc: 0.91
Running time: 1.3125 Second
同时可以画出准确度与时间随THRESHOLD的变化图如下：
![准确度与时间随THRESHOLD变化图](img/sklearn.png)
可以发现，其在THRESHOLD约为$1 \cdot 10^{-3}$时达到稳定，此时运行时间为1.3125s，准确率为0.91。
最终结果为：
w=[-2.802051483528587,0.06326673,0.07667397,0.18258601,0.60309771,-0.28708404,-0.20567921,0.4027636,-0.65930869,-0.84561177,-0.81467686,0.52399726,-0.67723609,-0.20361242 -0.44343857,-0.22124865,0.50415385,-0.41556441,-0.12376121,0.01310567,-0.51855263,-0.5626143,0.44617828,0.50301771,-0.07107305,-0.2538178,0.99309579,0.52277441,0.00215304,0.00112656]

## 3. SVM-gradient decent
### 3.1 实现方法
几乎相同于课堂上展示的伪代码，但是使用np与矩阵来进行计算，加速计算速度也减少代码量。
最终的损失函数为：
$$L(w,w_0|D)=\left\{
    \begin{aligned}
&\frac{\lambda}{2} ||w||^2 &y^{(l)}(w^Tx^{(l)}+w_0)\geq 1\\
&\frac{1}{N}\Sigma_{l=1}^N 1-y^{(l)}(w^Tx^{(l)}+w_0)+\frac{\lambda}{2} ||w||^2 & otherwise\\
\end{aligned}
\right.$$
其中，$\lambda$表示为下述PENALTY。
详细地，方法为：
1. 随机产生初始的w值。
2. 对于$w_1-w_N$，首先计算所有满足$y^{(l)}(w^Tx^{(l)}+w_0)\geq 1$对应的下标集合$I$，对于每个下标，计算$\Delta w_j=-\frac{1}{N} \cdot \Sigma_{i \in I}x_j^{(i)}\cdot y^{(i)}+PENALTY \cdot w_j$。对于$w_0$，$\Delta w_0 = -\Sigma_{i \in I}y^{(i)}$。
3. 更新$w_0=w_0-STEP\_SIZE \cdot \Delta w_0$，$w=w-STEP\_SIZE \cdot \Delta w$。
将测试数据输入模型，得到准确度。准确度的计算方式是，模型计算出的预测值与实际值差的绝对值的平均。同时由于初始随机性的选取，我将代码运行5次，准确度与时间计算取平均值。
### 3.2 参数
经过测试与选取，我选择了如下参数：
PENALTY = 0.001
THRESHOLD = 1e-9
STEP_SIZE = 0.0003
MAX_ITERATION = 8000
其中，选择PENALTY = 0.001是为了降低w的平方和的占比，选择THRESHOLD = 1e-9和MAX_ITERATION = 8000是为了在保证时间较短情况下增加准确度，选择STEP_SIZE = 0.0003是为了增加准确度，使得其缓慢迭代。
### 3.3 结果
根据上述参数配置，得到结果如下：
train_test: 0.9325
test_acc: 0.92
Running time: 2.78125 Second
同时可以画出当MAX_ITERATION = 8000时准确度与时间随THRESHOLD的变化图如下：
![准确度与时间随THRESHOLD变化图](img/gradient.png)
可以发现，其在THRESHOLD约为$1 \cdot 10^{-9}$时达到稳定。同时，设置为此值时，迭代次数差不多为8000（当设置为$1 \cdot 10^{-8}$时，迭代次数约为4000）。
同时，也可以画出准确度与时间随STEP_SIZE变化图如下：
![准确度与时间随STEP_SIZE变化图](img/gradient_step_size.png)
可以发现，其基本保持不变，因此我选择最大值，即STEP_SIZE = 0.0003。
画出准确度与时间随PENALTY变化图如下：
![准确度与时间随PENALTY变化图](img/gradient_penalty.png)
对于PENALTY来说，由于并未规定取多少，因此我取了可以使得运行时间最短同时准确率最高的0.001。
此时，运行时间为2.78125s，准确率为0.92。
最终结果为：
w=[-0.61687553,-2.26631034e-01,-1.03877327e-02,-2.37879242e-01,2.50725199e-01,-1.46651170e-01,9.40918150e-02,3.40519682e-02,-1.07140111e-01,-1.27404725e-01,4.30987033e-03,-4.80397888e-03,4.38860662e-02,-2.92633386e-02,9.89246583e-03,2.95924324e-02,-1.12967543e-04,4.02681566e-03]
### 3.4 变化
最终变化情况如图所示：
![变化情况](img/epoch.png)
画图如下所示：
![变化情况](img/gradient_epoch.png)

# 4. 对比与分析
### 4.1 时间对比
从时间上来看，达到相同的准确率需要的时间是sklearn小于使用gradient decent的SVM。我认为有两个原因：
1. 加速算法的使用。在计算中可以发现，sklearn的迭代次数远远多于8000次，而仍然能够达到快速，是由于用到了很多加速算法。例如多线程并行计算等。
2. 底层的不同。在sklearn中，很多函数的编写是使用C来编写的，相比python，C能提供极大的速度加成。
### 4.2 准确度对比
从准确度上看，两者大体相似。在足够迭代后，均能达到90%以上的准确度，且再增加迭代次数不会发生明显变化。这说明参数已经收敛，几乎达到了最佳的准确度。
### 4.3 结论
总之，手动编写的使用gradient decent方法的SVM也能达到与sklearn的SVM相似的效果，因此表现了代码的有效性。